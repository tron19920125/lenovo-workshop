{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8693432",
   "metadata": {},
   "source": [
    "# VM 使用 LLaMA-Factory Docker 部署 Llama3.1-8B 微调模型\n",
    "\n",
    "本 Jupyter Notebook 提供了在虚拟机（VM）上使用 LLaMA-Factory Docker 方式部署 Llama3.1-8B 模型的完整指南。本教程涵盖了从环境配置到模型部署和测试的所有步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e7df9c",
   "metadata": {},
   "source": [
    "## 概述\n",
    "\n",
    "**LLaMA-Factory** 是一个开源的大语言模型微调框架，提供了简单易用的工具来对 LLaMA、ChatGLM、Baichuan 等主流大模型进行高效的参数微调和训练\n",
    "\n",
    "**Llama3.1-8B** 是 Meta 发布的一个高效的大型语言模型，本教程将指导您如何在虚拟机（VM）上完整部署 Llama3.1-8B 模型，包括环境配置、模型下载、部署和测试等步骤。\n",
    "\n",
    "**Docker** 是一个容器化平台，将应用程序及其依赖环境打包成轻量级、可移植的容器，实现\"一次构建，处处运行\"的部署方式。\n",
    "\n",
    "**Azure Kubernetes Service (AKS)** 是微软 Azure 提供的托管 Kubernetes 服务，简化了容器化应用的部署、管理和扩展，无需用户管理 Kubernetes 控制平面。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bffd092",
   "metadata": {},
   "source": [
    "## 环境要求\n",
    "\n",
    "### 硬件要求\n",
    "- **CPU**: 至少 8 核心（推荐 16 核心或以上）\n",
    "- **内存**: 最少 16GB RAM（推荐 32GB 或以上）\n",
    "- **存储**: 至少 50GB 可用空间\n",
    "- **GPU**: NVIDIA GPU（至少 16GB 显存）\n",
    "\n",
    "### 软件要求\n",
    "- **操作系统**: Ubuntu 20.04/22.04 LTS 或 CentOS 7/8\n",
    "- **Python**: 3.8 或更高版本\n",
    "- **Docker**: 20.10 或更高版本\n",
    "- **CUDA**: 11.8 或更高版本（如果使用 GPU）\n",
    "\n",
    "### 系统要求\n",
    "- Azure 订阅（如果没有，请创建[免费账户](https://azure.microsoft.com/free/)）\n",
    "- Azure CLI 2.50.0 或更高版本\n",
    "- Docker 命令行工具\n",
    "- 至少 16GB RAM 的开发机器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af8915f",
   "metadata": {},
   "source": [
    "## 环境准备\n",
    "\n",
    "### 定义环境变量\n",
    "\n",
    "我们需要设置一些环境变量来管理我们的资源："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff4dbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 设置环境变量\n",
    "os.environ[\"RESOURCE_GROUP\"] = \"lenovo_vm_rg\"\n",
    "os.environ[\"LOCATION\"] = \"eastus2\"\n",
    "os.environ['VM_NAME'] = \"llama3_1_8b-vm\"\n",
    "os.environ['VM_SIZE'] = \"Standard_D4s_v3\"\n",
    "\n",
    "# 显示配置\n",
    "print(f\"Resource Group: {os.environ['RESOURCE_GROUP']}\")\n",
    "print(f\"Location: {os.environ['LOCATION']}\")\n",
    "print(f\"VM Name: {os.environ['VM_NAME']}\")\n",
    "print(f\"VM Size: {os.environ['VM_SIZE']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b43d238",
   "metadata": {},
   "source": [
    "## VM 环境准备\n",
    "\n",
    "### 1. 创建资源组\n",
    "\n",
    "Azure 中的资源组是一个逻辑容器, 它允许您根据资源的生命周期和安全需求将这些资源作为一个整体进行管理。资源组中的资源可以包括虚拟机、存储账户和虚拟网络等。\n",
    "\n",
    "以下使用 Azure CLI 创建 VM（需要先安装和配置 Azure CLI）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffc66bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "az group create \\\n",
    "    --name $RESOURCE_GROUP \\\n",
    "    --location $LOCATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09625ea",
   "metadata": {},
   "source": [
    "### 2. 创建 VM\n",
    "\n",
    "以下使用 Azure CLI 创建 VM（需要先安装和配置 Azure CLI）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9574365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 Azure CLI 创建 VM\n",
    "!az vm create \\\n",
    "  --resource-group $RESOURCE_GROUP \\\n",
    "  --name $VM_NAME \\\n",
    "  --image Ubuntu2204 \\\n",
    "  --size $VM_SIZE \\\n",
    "  --admin-username azureuser \\\n",
    "  --generate-ssh-keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c702549",
   "metadata": {},
   "source": [
    "### 2. 连接到 VM\n",
    "\n",
    "获取 VM 的 IP 地址后，使用 SSH 连接："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf45889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 替换为您的 VM IP 地址\n",
    "VM_IP=\"<VM-IP-ADDRESS>\"\n",
    "!ssh azureuser@$VM_IP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a43e36",
   "metadata": {},
   "source": [
    "> 下面的操作都将在虚拟机上进行，请将对应的命令拷贝到虚拟机上执行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6d8244",
   "metadata": {},
   "source": [
    "### 3. 更新系统\n",
    "\n",
    "运行以下命令更新系统并安装基础工具："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dd8f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新包列表和系统\n",
    "!sudo apt update && sudo apt upgrade -y\n",
    "\n",
    "# 安装基础工具\n",
    "!sudo apt install -y build-essential git wget curl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f665a9b0",
   "metadata": {},
   "source": [
    "## 安装依赖\n",
    "\n",
    "### 1. 安装 Docker 和 Docker Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc1470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装 Docker\n",
    "!curl -fsSL https://get.docker.com -o get-docker.sh\n",
    "!sudo sh get-docker.sh\n",
    "\n",
    "# 将当前用户添加到 docker 组\n",
    "!sudo usermod -aG docker $USER\n",
    "\n",
    "# 安装 Docker Compose\n",
    "!sudo curl -L \"https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\n",
    "!sudo chmod +x /usr/local/bin/docker-compose\n",
    "\n",
    "# 启动 Docker 服务\n",
    "!sudo systemctl start docker\n",
    "!sudo systemctl enable docker\n",
    "\n",
    "# 验证安装\n",
    "!docker --version\n",
    "!docker-compose --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da3e964",
   "metadata": {},
   "source": [
    "### 2. 安装 NVIDIA Container Toolkit（如果使用 GPU）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b3640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加 NVIDIA Docker 仓库\n",
    "!curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg\n",
    "!curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\\n",
    "  sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\\n",
    "  sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\n",
    "\n",
    "# 更新包列表并安装\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y nvidia-container-toolkit\n",
    "\n",
    "# 配置 Docker 以使用 NVIDIA runtime\n",
    "!sudo nvidia-ctk runtime configure --runtime=docker\n",
    "!sudo systemctl restart docker\n",
    "\n",
    "# 验证 GPU 支持\n",
    "!docker run --rm --gpus all nvidia/cuda:11.8-base-ubuntu22.04 nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92a0b43",
   "metadata": {},
   "source": [
    "### 3. 克隆 LLaMA-Factory 项目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0d2c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 克隆 LLaMA-Factory 项目\n",
    "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
    "!cd LLaMA-Factory\n",
    "\n",
    "# 查看项目结构\n",
    "!ls -la LLaMA-Factory/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fca248f",
   "metadata": {},
   "source": [
    "## 部署模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d6c5e4",
   "metadata": {},
   "source": [
    "### 1. 构建 Docker 镜像\n",
    "\n",
    "使用 `llama-factory` 官方 docker-compose 脚本启动服务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528af8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进入 LLaMA-Factory 目录\n",
    "%cd LLaMA-Factory/docker/docker-cuda/\n",
    "\n",
    "# 启动服务\n",
    "!docker compose up -d\n",
    "\n",
    "# 测试容器服务\n",
    "!docker compose exec llamafactory bash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ae1f08",
   "metadata": {},
   "source": [
    "### 2. 导入微调模型\n",
    "\n",
    "`Dockerfile` 通过 VOLUME 描述挂载目录，\n",
    "\n",
    "```yaml\n",
    "VOLUME [ \"/root/.cache/huggingface\", \"/root/.cache/modelscope\", \"/app/data\", \"/app/output\" ]\n",
    "```\n",
    "\n",
    "- `/root/.cache/huggingface`：huggingface 模型缓存目录\n",
    "- `/root/.cache/modelscope`： modelscope 模型缓存目录\n",
    "\n",
    "用户根据需要将微调模型放入到对应目录当中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c3ecde",
   "metadata": {},
   "source": [
    "### 3. 创建 API 服务启动脚本\n",
    "\n",
    "创建一个脚本来启动 LLaMA-Factory 的 API 服务："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea3d5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile start_llama3_api.sh\n",
    "#!/bin/bash\n",
    "\n",
    "# 启动 LLaMA-Factory API 服务\n",
    "docker exec -d llamafactory llamafactory-cli api \\\n",
    "    examples/inference/llama3.yaml \\\n",
    "    infer_backend=vllm \\\n",
    "    vllm_enforce_eager=true \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8000\n",
    "\n",
    "echo \"Llama3 API 服务已启动在端口 8000\"\n",
    "echo \"访问 http://localhost:8000/docs 查看 API 文档\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caebc5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给脚本添加执行权限\n",
    "!chmod +x start_api.sh\n",
    "\n",
    "# 启动 API 服务\n",
    "!./start_api.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69c0637",
   "metadata": {},
   "source": [
    "## 测试验证\n",
    "\n",
    "### 1. 检查容器状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ddd108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查容器运行状态\n",
    "!docker ps\n",
    "\n",
    "# 查看容器日志\n",
    "!docker logs llamafactory\n",
    "\n",
    "# 检查 GPU 使用情况（如果有 GPU）\n",
    "!docker exec llamafactory nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96908f9",
   "metadata": {},
   "source": [
    "### 2. 测试 API 接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209c54ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试健康检查\n",
    "!curl http://localhost:8000/v1/models\n",
    "\n",
    "# 测试聊天接口\n",
    "%%bash\n",
    "curl -X POST \"http://localhost:8000/v1/chat/completions\" \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\n",
    "       \"model\": \"microsoft/Llama3_1_9b\",\n",
    "       \"messages\": [\n",
    "         {\n",
    "           \"role\": \"user\",\n",
    "           \"content\": \"什么是人工智能？\"\n",
    "         }\n",
    "       ],\n",
    "       \"max_tokens\": 512,\n",
    "       \"temperature\": 0.7\n",
    "     }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f17a0f",
   "metadata": {},
   "source": [
    "### 3. 监控和日志"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c83a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 监控容器资源使用情况\n",
    "!docker stats llamafactory-phi4\n",
    "\n",
    "# 查看详细的容器信息\n",
    "!docker inspect llamafactory-phi4\n",
    "\n",
    "# 实时查看日志\n",
    "!docker logs -f llamafactory-phi4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00e891d",
   "metadata": {},
   "source": [
    "## 常见问题\n",
    "\n",
    "### 1. Docker 容器内存不足"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61ea528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增加 Docker 容器的内存限制\n",
    "!docker update --memory=32g --memory-swap=64g llamafactory\n",
    "\n",
    "# 或者在 docker-compose.yml 中设置\n",
    "# deploy:\n",
    "#   resources:\n",
    "#     limits:\n",
    "#       memory: 32G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb96c394",
   "metadata": {},
   "source": [
    "### 2. GPU 在容器中不可用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e6199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查 NVIDIA Container Toolkit 是否正确安装\n",
    "!docker run --rm --gpus all nvidia/cuda:11.8-base-ubuntu22.04 nvidia-smi\n",
    "\n",
    "# 重新配置 Docker 以支持 GPU\n",
    "!sudo nvidia-ctk runtime configure --runtime=docker\n",
    "!sudo systemctl restart docker\n",
    "\n",
    "# 确保 docker-compose.yml 中包含 GPU 配置\n",
    "# deploy:\n",
    "#   resources:\n",
    "#     reservations:\n",
    "#       devices:\n",
    "#         - driver: nvidia\n",
    "#           count: all\n",
    "#           capabilities: [gpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb9451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在容器内设置 Hugging Face 镜像\n",
    "!docker exec llamafactory bash -c \"export HF_ENDPOINT=https://hf-mirror.com\"\n",
    "\n",
    "# 或者在 docker-compose.yml 中设置环境变量\n",
    "# environment:\n",
    "#   - HF_ENDPOINT=https://hf-mirror.com\n",
    "\n",
    "# 使用代理下载（如果需要）\n",
    "# environment:\n",
    "#   - https_proxy=http://your-proxy:port\n",
    "#   - http_proxy=http://your-proxy:port"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
