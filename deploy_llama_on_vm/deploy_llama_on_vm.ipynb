{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8693432",
   "metadata": {},
   "source": [
    "# VM 使用 LLaMA-Factory Docker 部署 Llama3.1-8B 微调模型\n",
    "\n",
    "本 Jupyter Notebook 提供了在虚拟机（VM）上使用 LLaMA-Factory Docker 方式部署 Llama3.1-8B 模型的完整指南。本教程涵盖了从环境配置到模型部署和测试的所有步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe41467",
   "metadata": {},
   "source": [
    "## 目录\n",
    "1. [概述](#概述)\n",
    "2. [环境要求](#环境要求)\n",
    "3. [VM 环境准备](#vm-环境准备)\n",
    "4. [安装依赖](#安装依赖)\n",
    "5. [下载模型](#下载模型)\n",
    "6. [部署模型](#部署模型)\n",
    "7. [测试验证](#测试验证)\n",
    "8. [性能优化](#性能优化)\n",
    "9. [常见问题](#常见问题)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e7df9c",
   "metadata": {},
   "source": [
    "## 概述\n",
    "\n",
    "Llama3.1-8B 是 Meta 发布的一个高效的大型语言模型，本教程将指导您如何在虚拟机（VM）上完整部署 Llama3.1-8B 模型，包括环境配置、模型下载、部署和测试等步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bffd092",
   "metadata": {},
   "source": [
    "## 环境要求\n",
    "\n",
    "### 硬件要求\n",
    "- **CPU**: 至少 8 核心（推荐 16 核心或以上）\n",
    "- **内存**: 最少 16GB RAM（推荐 32GB 或以上）\n",
    "- **存储**: 至少 50GB 可用空间\n",
    "- **GPU**: 可选，但强烈推荐使用 NVIDIA GPU（至少 8GB 显存）\n",
    "\n",
    "### 软件要求\n",
    "- **操作系统**: Ubuntu 20.04/22.04 LTS 或 CentOS 7/8\n",
    "- **Python**: 3.8 或更高版本\n",
    "- **Docker**: 20.10 或更高版本\n",
    "- **CUDA**: 11.8 或更高版本（如果使用 GPU）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af8915f",
   "metadata": {},
   "source": [
    "## 环境准备\n",
    "\n",
    "### 定义环境变量\n",
    "\n",
    "我们需要设置一些环境变量来管理我们的资源："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff4dbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 设置环境变量\n",
    "os.environ[\"RESOURCE_GROUP\"] = \"lenovo_vm_rg\"\n",
    "os.environ[\"LOCATION\"] = \"eastus2\"\n",
    "os.environ['VM_NAME'] = \"llama3_1_8b-vm\"\n",
    "os.environ['VM_SIZE'] = \"Standard_D4s_v3\"\n",
    "\n",
    "# 显示配置\n",
    "print(f\"Resource Group: {os.environ['RESOURCE_GROUP']}\")\n",
    "print(f\"Location: {os.environ['LOCATION']}\")\n",
    "print(f\"VM Name: {os.environ['VM_NAME']}\")\n",
    "print(f\"VM Size: {os.environ['VM_SIZE']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b43d238",
   "metadata": {},
   "source": [
    "## VM 环境准备\n",
    "\n",
    "### 1. 创建资源组\n",
    "\n",
    "以下使用 Azure CLI 创建 VM（需要先安装和配置 Azure CLI）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffc66bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "az group create \\\n",
    "    --name $RESOURCE_GROUP \\\n",
    "    --location $LOCATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09625ea",
   "metadata": {},
   "source": [
    "### 2. 创建虚拟机\n",
    "\n",
    "以下使用 Azure CLI 创建 VM（需要先安装和配置 Azure CLI）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9574365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: (ResourceGroupNotFound) Resource group 'test-vm-rg' could not be found.\n",
      "Code: ResourceGroupNotFound\n",
      "Message: Resource group 'test-vm-rg' could not be found.\n"
     ]
    }
   ],
   "source": [
    "# 使用 Azure CLI 创建 VM\n",
    "!az vm create \\\n",
    "  --resource-group $RESOURCE_GROUP \\\n",
    "  --name $VM_NAME \\\n",
    "  --image Ubuntu2204 \\\n",
    "  --size $VM_SIZE \\\n",
    "  --admin-username azureuser \\\n",
    "  --generate-ssh-keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c702549",
   "metadata": {},
   "source": [
    "### 2. 连接到 VM\n",
    "\n",
    "获取 VM 的 IP 地址后，使用 SSH 连接："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf45889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 替换为您的 VM IP 地址\n",
    "VM_IP=\"<VM-IP-ADDRESS>\"\n",
    "!ssh azureuser@$VM_IP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a43e36",
   "metadata": {},
   "source": [
    "> 下面的操作都将在虚拟机上进行，请将对应的命令拷贝到虚拟机上执行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6d8244",
   "metadata": {},
   "source": [
    "### 3. 更新系统\n",
    "\n",
    "运行以下命令更新系统并安装基础工具："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dd8f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新包列表和系统\n",
    "!sudo apt update && sudo apt upgrade -y\n",
    "\n",
    "# 安装基础工具\n",
    "!sudo apt install -y build-essential git wget curl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f665a9b0",
   "metadata": {},
   "source": [
    "## 安装依赖\n",
    "\n",
    "### 1. 安装 Docker 和 Docker Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc1470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装 Docker\n",
    "!curl -fsSL https://get.docker.com -o get-docker.sh\n",
    "!sudo sh get-docker.sh\n",
    "\n",
    "# 将当前用户添加到 docker 组\n",
    "!sudo usermod -aG docker $USER\n",
    "\n",
    "# 安装 Docker Compose\n",
    "!sudo curl -L \"https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\n",
    "!sudo chmod +x /usr/local/bin/docker-compose\n",
    "\n",
    "# 启动 Docker 服务\n",
    "!sudo systemctl start docker\n",
    "!sudo systemctl enable docker\n",
    "\n",
    "# 验证安装\n",
    "!docker --version\n",
    "!docker-compose --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da3e964",
   "metadata": {},
   "source": [
    "### 2. 安装 NVIDIA Container Toolkit（如果使用 GPU）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b3640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加 NVIDIA Docker 仓库\n",
    "!curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg\n",
    "!curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\\n",
    "  sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\\n",
    "  sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\n",
    "\n",
    "# 更新包列表并安装\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y nvidia-container-toolkit\n",
    "\n",
    "# 配置 Docker 以使用 NVIDIA runtime\n",
    "!sudo nvidia-ctk runtime configure --runtime=docker\n",
    "!sudo systemctl restart docker\n",
    "\n",
    "# 验证 GPU 支持\n",
    "!docker run --rm --gpus all nvidia/cuda:11.8-base-ubuntu22.04 nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92a0b43",
   "metadata": {},
   "source": [
    "### 3. 克隆 LLaMA-Factory 项目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0d2c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 克隆 LLaMA-Factory 项目\n",
    "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
    "!cd LLaMA-Factory\n",
    "\n",
    "# 查看项目结构\n",
    "!ls -la LLaMA-Factory/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fca248f",
   "metadata": {},
   "source": [
    "## 配置 LLaMA-Factory\n",
    "\n",
    "### 1. 创建 Docker Compose 配置文件\n",
    "\n",
    "我们需要创建一个 `docker-compose.yml` 文件来配置 LLaMA-Factory 服务："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d6c5e4",
   "metadata": {},
   "source": [
    "## 部署模型\n",
    "\n",
    "### 1. 构建 Docker 镜像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528af8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进入 LLaMA-Factory 目录\n",
    "%cd LLaMA-Factory\n",
    "\n",
    "# 构建 Docker 镜像\n",
    "!docker-compose build\n",
    "\n",
    "# 或者直接使用预构建的镜像\n",
    "!docker pull registry.cn-hangzhou.aliyuncs.com/modelscope-repo/llamafactory:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ae1f08",
   "metadata": {},
   "source": [
    "### 2. 启动容器并下载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0cfb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 启动容器\n",
    "!docker-compose up -d\n",
    "\n",
    "# 进入容器\n",
    "!docker exec -it llamafactory-phi4 bash\n",
    "\n",
    "# 在容器内下载 Phi-4 模型\n",
    "# 这个命令需要在容器内执行\n",
    "!echo \"huggingface-cli download microsoft/Phi-4 --local-dir /app/models/Phi-4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c3ecde",
   "metadata": {},
   "source": [
    "### 3. 创建 API 服务启动脚本\n",
    "\n",
    "创建一个脚本来启动 LLaMA-Factory 的 API 服务："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea3d5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile start_phi4_api.sh\n",
    "#!/bin/bash\n",
    "\n",
    "# 启动 LLaMA-Factory API 服务\n",
    "docker exec -d llamafactory-phi4 llamafactory-cli api \\\n",
    "    --model_name_or_path microsoft/Phi-4 \\\n",
    "    --template default \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8000\n",
    "\n",
    "echo \"Phi-4 API 服务已启动在端口 8000\"\n",
    "echo \"访问 http://localhost:8000/docs 查看 API 文档\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9187514",
   "metadata": {},
   "source": [
    "### 4. 创建 Web UI 启动脚本\n",
    "\n",
    "创建一个脚本来启动 LLaMA-Factory 的 Web UI："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eff425",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile start_phi4_webui.sh\n",
    "#!/bin/bash\n",
    "\n",
    "# 启动 LLaMA-Factory Web UI\n",
    "docker exec -d llamafactory-phi4 llamafactory-cli webui \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 7860\n",
    "\n",
    "echo \"Phi-4 Web UI 已启动在端口 7860\"\n",
    "echo \"访问 http://localhost:7860 使用 Web 界面\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caebc5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给脚本添加执行权限\n",
    "!chmod +x start_phi4_api.sh\n",
    "!chmod +x start_phi4_webui.sh\n",
    "\n",
    "# 启动 API 服务\n",
    "!./start_phi4_api.sh\n",
    "\n",
    "# 启动 Web UI\n",
    "!./start_phi4_webui.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69c0637",
   "metadata": {},
   "source": [
    "## 测试验证\n",
    "\n",
    "### 1. 检查容器状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ddd108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查容器运行状态\n",
    "!docker ps\n",
    "\n",
    "# 查看容器日志\n",
    "!docker logs llamafactory-phi4\n",
    "\n",
    "# 检查 GPU 使用情况（如果有 GPU）\n",
    "!docker exec llamafactory-phi4 nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96908f9",
   "metadata": {},
   "source": [
    "### 2. 测试 API 接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209c54ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试健康检查\n",
    "!curl http://localhost:8000/v1/models\n",
    "\n",
    "# 测试聊天接口\n",
    "%%bash\n",
    "curl -X POST \"http://localhost:8000/v1/chat/completions\" \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\n",
    "       \"model\": \"microsoft/Phi-4\",\n",
    "       \"messages\": [\n",
    "         {\n",
    "           \"role\": \"user\",\n",
    "           \"content\": \"什么是人工智能？\"\n",
    "         }\n",
    "       ],\n",
    "       \"max_tokens\": 512,\n",
    "       \"temperature\": 0.7\n",
    "     }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eb3018",
   "metadata": {},
   "source": [
    "## 性能优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1604ae4f",
   "metadata": {},
   "source": [
    "### 1. 模型量化配置\n",
    "\n",
    "为了减少内存使用，可以使用量化技术："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d009fd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile start_phi4_quantized.sh\n",
    "#!/bin/bash\n",
    "\n",
    "# 启动量化版本的 Phi-4 API 服务\n",
    "docker exec -d llamafactory-phi4 llamafactory-cli api \\\n",
    "    --model_name_or_path microsoft/Phi-4 \\\n",
    "    --template default \\\n",
    "    --quantization_bit 4 \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8000\n",
    "\n",
    "echo \"量化版 Phi-4 API 服务已启动在端口 8000\"\n",
    "echo \"使用 4-bit 量化以减少内存使用\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f17a0f",
   "metadata": {},
   "source": [
    "### 3. 监控和日志"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c83a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 监控容器资源使用情况\n",
    "!docker stats llamafactory-phi4\n",
    "\n",
    "# 查看详细的容器信息\n",
    "!docker inspect llamafactory-phi4\n",
    "\n",
    "# 实时查看日志\n",
    "!docker logs -f llamafactory-phi4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00e891d",
   "metadata": {},
   "source": [
    "## 常见问题\n",
    "\n",
    "### 1. Docker 容器内存不足"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61ea528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增加 Docker 容器的内存限制\n",
    "!docker update --memory=32g --memory-swap=64g llamafactory-phi4\n",
    "\n",
    "# 或者在 docker-compose.yml 中设置\n",
    "# deploy:\n",
    "#   resources:\n",
    "#     limits:\n",
    "#       memory: 32G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb96c394",
   "metadata": {},
   "source": [
    "### 2. GPU 在容器中不可用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e6199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查 NVIDIA Container Toolkit 是否正确安装\n",
    "!docker run --rm --gpus all nvidia/cuda:11.8-base-ubuntu22.04 nvidia-smi\n",
    "\n",
    "# 重新配置 Docker 以支持 GPU\n",
    "!sudo nvidia-ctk runtime configure --runtime=docker\n",
    "!sudo systemctl restart docker\n",
    "\n",
    "# 确保 docker-compose.yml 中包含 GPU 配置\n",
    "# deploy:\n",
    "#   resources:\n",
    "#     reservations:\n",
    "#       devices:\n",
    "#         - driver: nvidia\n",
    "#           count: all\n",
    "#           capabilities: [gpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb9451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在容器内设置 Hugging Face 镜像\n",
    "!docker exec llamafactory-phi4 bash -c \"export HF_ENDPOINT=https://hf-mirror.com\"\n",
    "\n",
    "# 或者在 docker-compose.yml 中设置环境变量\n",
    "# environment:\n",
    "#   - HF_ENDPOINT=https://hf-mirror.com\n",
    "\n",
    "# 使用代理下载（如果需要）\n",
    "# environment:\n",
    "#   - https_proxy=http://your-proxy:port\n",
    "#   - http_proxy=http://your-proxy:port"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41784616",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本教程详细介绍了如何在 VM 上使用 LLaMA-Factory Docker 方式部署 Phi-4 模型：\n",
    "\n",
    "### 主要步骤：\n",
    "1. **环境准备**：安装 Docker 和 NVIDIA Container Toolkit\n",
    "2. **项目配置**：克隆 LLaMA-Factory 并配置 Docker Compose\n",
    "3. **模型部署**：构建镜像、启动容器、下载模型\n",
    "4. **服务启动**：启动 API 服务和 Web UI\n",
    "5. **测试验证**：通过 API 和客户端测试模型功能\n",
    "\n",
    "### 优势：\n",
    "- **环境隔离**：Docker 容器提供干净的运行环境\n",
    "- **易于管理**：通过 Docker Compose 统一管理服务\n",
    "- **可扩展性**：支持 GPU 加速和资源优化\n",
    "- **便于部署**：一键启动所有服务\n",
    "\n",
    "### 注意事项：\n",
    "- 确保 VM 有足够的内存和存储空间\n",
    "- 正确配置 GPU 支持（如果使用）\n",
    "- 合理设置资源限制和优化参数\n",
    "- 定期监控容器状态和资源使用情况"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
