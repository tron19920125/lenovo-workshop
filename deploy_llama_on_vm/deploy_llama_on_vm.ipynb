{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8693432",
   "metadata": {},
   "source": [
    "# VM 使用 LLaMA-Factory Docker 部署 Phi-4 模型完整教程\n",
    "\n",
    "本 Jupyter Notebook 提供了在虚拟机（VM）上使用 LLaMA-Factory Docker 方式部署 Phi-4 模型的完整指南。本教程涵盖了从环境配置到模型部署和测试的所有步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe41467",
   "metadata": {},
   "source": [
    "## 目录\n",
    "1. [概述](#概述)\n",
    "2. [环境要求](#环境要求)\n",
    "3. [VM 环境准备](#vm-环境准备)\n",
    "4. [安装依赖](#安装依赖)\n",
    "5. [下载模型](#下载模型)\n",
    "6. [部署模型](#部署模型)\n",
    "7. [测试验证](#测试验证)\n",
    "8. [性能优化](#性能优化)\n",
    "9. [常见问题](#常见问题)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e7df9c",
   "metadata": {},
   "source": [
    "## 概述\n",
    "\n",
    "Phi-4 是微软发布的一个高效的小型语言模型，本教程将指导您如何在虚拟机（VM）上完整部署 Phi-4 模型，包括环境配置、模型下载、部署和测试等步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bffd092",
   "metadata": {},
   "source": [
    "## 环境要求\n",
    "\n",
    "### 硬件要求\n",
    "- **CPU**: 至少 8 核心（推荐 16 核心或以上）\n",
    "- **内存**: 最少 16GB RAM（推荐 32GB 或以上）\n",
    "- **存储**: 至少 50GB 可用空间\n",
    "- **GPU**: 可选，但强烈推荐使用 NVIDIA GPU（至少 8GB 显存）\n",
    "\n",
    "### 软件要求\n",
    "- **操作系统**: Ubuntu 20.04/22.04 LTS 或 CentOS 7/8\n",
    "- **Python**: 3.8 或更高版本\n",
    "- **CUDA**: 11.8 或更高版本（如果使用 GPU）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af8915f",
   "metadata": {},
   "source": [
    "## 环境准备\n",
    "\n",
    "### 定义环境变量\n",
    "\n",
    "我们需要设置一些环境变量来管理我们的资源："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff4dbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# 设置环境变量\n",
    "os.environ[\"RESOURCE_GROUP\"] = \"lenovo_vm_rg\"\n",
    "os.environ[\"LOCATION\"] = \"eastus2\"\n",
    "os.environ['VM_NAME'] = \"llama3_1_8b-vm\"\n",
    "os.environ['VM_SIZE'] = \"Standard_D4s_v3\"\n",
    "\n",
    "# 显示配置\n",
    "print(f\"Resource Group: {os.environ['RESOURCE_GROUP']}\")\n",
    "print(f\"Location: {os.environ['LOCATION']}\")\n",
    "print(f\"VM Name: {os.environ['VM_NAME']}\")\n",
    "print(f\"VM Size: {os.environ['VM_SIZE']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b43d238",
   "metadata": {},
   "source": [
    "## VM 环境准备\n",
    "\n",
    "### 1. 创建资源组\n",
    "\n",
    "以下使用 Azure CLI 创建 VM（需要先安装和配置 Azure CLI）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffc66bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "az group create \\\n",
    "    --name $RESOURCE_GROUP \\\n",
    "    --location $LOCATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09625ea",
   "metadata": {},
   "source": [
    "### 2. 创建虚拟机\n",
    "\n",
    "以下使用 Azure CLI 创建 VM（需要先安装和配置 Azure CLI）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9574365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: (ResourceGroupNotFound) Resource group 'test-vm-rg' could not be found.\n",
      "Code: ResourceGroupNotFound\n",
      "Message: Resource group 'test-vm-rg' could not be found.\n"
     ]
    }
   ],
   "source": [
    "# 使用 Azure CLI 创建 VM\n",
    "!az vm create \\\n",
    "  --resource-group $RESOURCE_GROUP \\\n",
    "  --name $VM_NAME \\\n",
    "  --image Ubuntu2204 \\\n",
    "  --size $VM_SIZE \\\n",
    "  --admin-username azureuser \\\n",
    "  --generate-ssh-keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c702549",
   "metadata": {},
   "source": [
    "### 2. 连接到 VM\n",
    "\n",
    "获取 VM 的 IP 地址后，使用 SSH 连接："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf45889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 替换为您的 VM IP 地址\n",
    "VM_IP=\"<VM-IP-ADDRESS>\"\n",
    "!ssh azureuser@$VM_IP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6d8244",
   "metadata": {},
   "source": [
    "### 3. 更新系统\n",
    "\n",
    "运行以下命令更新系统并安装基础工具："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dd8f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新包列表和系统\n",
    "!sudo apt update && sudo apt upgrade -y\n",
    "\n",
    "# 安装基础工具\n",
    "!sudo apt install -y build-essential git wget curl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f665a9b0",
   "metadata": {},
   "source": [
    "## 安装依赖\n",
    "\n",
    "### 1. 安装 Docker 和 Docker Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc1470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装 Docker\n",
    "!curl -fsSL https://get.docker.com -o get-docker.sh\n",
    "!sudo sh get-docker.sh\n",
    "\n",
    "# 将当前用户添加到 docker 组\n",
    "!sudo usermod -aG docker $USER\n",
    "\n",
    "# 安装 Docker Compose\n",
    "!sudo curl -L \"https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\n",
    "!sudo chmod +x /usr/local/bin/docker-compose\n",
    "\n",
    "# 启动 Docker 服务\n",
    "!sudo systemctl start docker\n",
    "!sudo systemctl enable docker\n",
    "\n",
    "# 验证安装\n",
    "!docker --version\n",
    "!docker-compose --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da3e964",
   "metadata": {},
   "source": [
    "### 2. 安装 NVIDIA Container Toolkit（如果使用 GPU）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b3640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加 NVIDIA Docker 仓库\n",
    "!curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg\n",
    "!curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\\n",
    "  sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\\n",
    "  sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\n",
    "\n",
    "# 更新包列表并安装\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y nvidia-container-toolkit\n",
    "\n",
    "# 配置 Docker 以使用 NVIDIA runtime\n",
    "!sudo nvidia-ctk runtime configure --runtime=docker\n",
    "!sudo systemctl restart docker\n",
    "\n",
    "# 验证 GPU 支持\n",
    "!docker run --rm --gpus all nvidia/cuda:11.8-base-ubuntu22.04 nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92a0b43",
   "metadata": {},
   "source": [
    "### 3. 克隆 LLaMA-Factory 项目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0d2c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 克隆 LLaMA-Factory 项目\n",
    "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
    "!cd LLaMA-Factory\n",
    "\n",
    "# 查看项目结构\n",
    "!ls -la LLaMA-Factory/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fca248f",
   "metadata": {},
   "source": [
    "## 配置 LLaMA-Factory\n",
    "\n",
    "### 1. 创建 Docker Compose 配置文件\n",
    "\n",
    "我们需要创建一个 `docker-compose.yml` 文件来配置 LLaMA-Factory 服务："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b367b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile LLaMA-Factory/docker-compose.yml\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  llamafactory:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: Dockerfile\n",
    "    container_name: llamafactory-phi4\n",
    "    ports:\n",
    "      - \"7860:7860\"  # Gradio Web UI\n",
    "      - \"8000:8000\"  # API 服务\n",
    "    volumes:\n",
    "      - ./data:/app/data\n",
    "      - ./saves:/app/saves\n",
    "      - ./models:/app/models\n",
    "      - ./logs:/app/logs\n",
    "    environment:\n",
    "      - CUDA_VISIBLE_DEVICES=0\n",
    "      - HF_HOME=/app/models\n",
    "      - HF_HUB_CACHE=/app/models\n",
    "    deploy:\n",
    "      resources:\n",
    "        reservations:\n",
    "          devices:\n",
    "            - driver: nvidia\n",
    "              count: all\n",
    "              capabilities: [gpu]\n",
    "    stdin_open: true\n",
    "    tty: true\n",
    "    command: bash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7e16c2",
   "metadata": {},
   "source": [
    "### 2. 创建模型配置文件\n",
    "\n",
    "创建 Phi-4 模型的配置文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483b8096",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile LLaMA-Factory/examples/inference/phi4_config.yaml\n",
    "### model\n",
    "model_name_or_path: microsoft/Phi-4\n",
    "adapter_name_or_path: null\n",
    "\n",
    "### method\n",
    "finetuning_type: lora\n",
    "quantization_bit: null\n",
    "\n",
    "### ddp\n",
    "ddp_timeout: 180000000\n",
    "\n",
    "### output\n",
    "output_dir: saves/Phi-4/lora/sft\n",
    "\n",
    "### train\n",
    "cutoff_len: 1024\n",
    "learning_rate: 5.0e-5\n",
    "num_train_epochs: 3.0\n",
    "max_samples: 1000\n",
    "per_device_train_batch_size: 2\n",
    "gradient_accumulation_steps: 8\n",
    "lr_scheduler_type: cosine\n",
    "max_grad_norm: 1.0\n",
    "logging_steps: 5\n",
    "save_steps: 100\n",
    "warmup_steps: 0\n",
    "neftune_noise_alpha: null\n",
    "optim: adamw_torch\n",
    "resize_vocab: false\n",
    "packing: false\n",
    "upsampling: false\n",
    "\n",
    "### eval\n",
    "val_size: 0.1\n",
    "per_device_eval_batch_size: 1\n",
    "eval_strategy: steps\n",
    "eval_steps: 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d6c5e4",
   "metadata": {},
   "source": [
    "## 部署模型\n",
    "\n",
    "### 1. 构建 Docker 镜像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528af8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进入 LLaMA-Factory 目录\n",
    "%cd LLaMA-Factory\n",
    "\n",
    "# 构建 Docker 镜像\n",
    "!docker-compose build\n",
    "\n",
    "# 或者直接使用预构建的镜像\n",
    "!docker pull registry.cn-hangzhou.aliyuncs.com/modelscope-repo/llamafactory:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ae1f08",
   "metadata": {},
   "source": [
    "### 2. 启动容器并下载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0cfb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 启动容器\n",
    "!docker-compose up -d\n",
    "\n",
    "# 进入容器\n",
    "!docker exec -it llamafactory-phi4 bash\n",
    "\n",
    "# 在容器内下载 Phi-4 模型\n",
    "# 这个命令需要在容器内执行\n",
    "!echo \"huggingface-cli download microsoft/Phi-4 --local-dir /app/models/Phi-4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c3ecde",
   "metadata": {},
   "source": [
    "### 3. 创建 API 服务启动脚本\n",
    "\n",
    "创建一个脚本来启动 LLaMA-Factory 的 API 服务："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea3d5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile start_phi4_api.sh\n",
    "#!/bin/bash\n",
    "\n",
    "# 启动 LLaMA-Factory API 服务\n",
    "docker exec -d llamafactory-phi4 llamafactory-cli api \\\n",
    "    --model_name_or_path microsoft/Phi-4 \\\n",
    "    --template default \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8000\n",
    "\n",
    "echo \"Phi-4 API 服务已启动在端口 8000\"\n",
    "echo \"访问 http://localhost:8000/docs 查看 API 文档\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9187514",
   "metadata": {},
   "source": [
    "### 4. 创建 Web UI 启动脚本\n",
    "\n",
    "创建一个脚本来启动 LLaMA-Factory 的 Web UI："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eff425",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile start_phi4_webui.sh\n",
    "#!/bin/bash\n",
    "\n",
    "# 启动 LLaMA-Factory Web UI\n",
    "docker exec -d llamafactory-phi4 llamafactory-cli webui \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 7860\n",
    "\n",
    "echo \"Phi-4 Web UI 已启动在端口 7860\"\n",
    "echo \"访问 http://localhost:7860 使用 Web 界面\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caebc5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给脚本添加执行权限\n",
    "!chmod +x start_phi4_api.sh\n",
    "!chmod +x start_phi4_webui.sh\n",
    "\n",
    "# 启动 API 服务\n",
    "!./start_phi4_api.sh\n",
    "\n",
    "# 启动 Web UI\n",
    "!./start_phi4_webui.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69c0637",
   "metadata": {},
   "source": [
    "## 测试验证\n",
    "\n",
    "### 1. 检查容器状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ddd108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查容器运行状态\n",
    "!docker ps\n",
    "\n",
    "# 查看容器日志\n",
    "!docker logs llamafactory-phi4\n",
    "\n",
    "# 检查 GPU 使用情况（如果有 GPU）\n",
    "!docker exec llamafactory-phi4 nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96908f9",
   "metadata": {},
   "source": [
    "### 2. 测试 API 接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209c54ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试健康检查\n",
    "!curl http://localhost:8000/v1/models\n",
    "\n",
    "# 测试聊天接口\n",
    "%%bash\n",
    "curl -X POST \"http://localhost:8000/v1/chat/completions\" \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\n",
    "       \"model\": \"microsoft/Phi-4\",\n",
    "       \"messages\": [\n",
    "         {\n",
    "           \"role\": \"user\",\n",
    "           \"content\": \"什么是人工智能？\"\n",
    "         }\n",
    "       ],\n",
    "       \"max_tokens\": 512,\n",
    "       \"temperature\": 0.7\n",
    "     }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebcf351",
   "metadata": {},
   "source": [
    "### 3. Python 客户端测试\n",
    "\n",
    "创建一个 Python 客户端来测试 API："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3fd7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_phi4_client.py\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def test_phi4_api():\n",
    "    \"\"\"测试 Phi-4 API\"\"\"\n",
    "    \n",
    "    # API 端点\n",
    "    url = \"http://localhost:8000/v1/chat/completions\"\n",
    "    \n",
    "    # 测试用例\n",
    "    test_messages = [\n",
    "        [{\"role\": \"user\", \"content\": \"请解释量子计算的基本原理\"}],\n",
    "        [{\"role\": \"user\", \"content\": \"写一个 Python 函数计算斐波那契数列\"}],\n",
    "        [{\"role\": \"user\", \"content\": \"描述一下未来的智能城市\"}]\n",
    "    ]\n",
    "    \n",
    "    for i, messages in enumerate(test_messages, 1):\n",
    "        print(f\"\\n=== 测试 {i} ===\")\n",
    "        print(f\"输入: {messages[0]['content']}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                url,\n",
    "                json={\n",
    "                    \"model\": \"microsoft/Phi-4\",\n",
    "                    \"messages\": messages,\n",
    "                    \"max_tokens\": 512,\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"stream\": False\n",
    "                },\n",
    "                timeout=60\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                content = result['choices'][0]['message']['content']\n",
    "                print(f\"输出: {content}\")\n",
    "            else:\n",
    "                print(f\"错误: {response.status_code} - {response.text}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"请求失败: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_phi4_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2d3b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装 requests 库\n",
    "%pip install requests\n",
    "\n",
    "# 运行测试客户端\n",
    "!python test_phi4_client.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eb3018",
   "metadata": {},
   "source": [
    "## 性能优化\n",
    "\n",
    "### 1. Docker 容器资源优化\n",
    "\n",
    "为了获得更好的性能，可以调整 Docker 容器的资源配置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d978495",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile LLaMA-Factory/docker-compose-optimized.yml\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  llamafactory:\n",
    "    image: registry.cn-hangzhou.aliyuncs.com/modelscope-repo/llamafactory:latest\n",
    "    container_name: llamafactory-phi4-optimized\n",
    "    ports:\n",
    "      - \"7860:7860\"\n",
    "      - \"8000:8000\"\n",
    "    volumes:\n",
    "      - ./data:/app/data\n",
    "      - ./saves:/app/saves\n",
    "      - ./models:/app/models\n",
    "      - ./logs:/app/logs\n",
    "      - /tmp:/tmp  # 临时文件目录\n",
    "    environment:\n",
    "      - CUDA_VISIBLE_DEVICES=0\n",
    "      - HF_HOME=/app/models\n",
    "      - HF_HUB_CACHE=/app/models\n",
    "      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128\n",
    "      - OMP_NUM_THREADS=8\n",
    "    deploy:\n",
    "      resources:\n",
    "        limits:\n",
    "          memory: 32G\n",
    "          cpus: '16'\n",
    "        reservations:\n",
    "          memory: 16G\n",
    "          cpus: '8'\n",
    "          devices:\n",
    "            - driver: nvidia\n",
    "              count: all\n",
    "              capabilities: [gpu]\n",
    "    shm_size: 8gb  # 增加共享内存\n",
    "    ulimits:\n",
    "      memlock: -1\n",
    "      stack: 67108864\n",
    "    stdin_open: true\n",
    "    tty: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1604ae4f",
   "metadata": {},
   "source": [
    "### 2. 模型量化配置\n",
    "\n",
    "为了减少内存使用，可以使用量化技术："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d009fd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile start_phi4_quantized.sh\n",
    "#!/bin/bash\n",
    "\n",
    "# 启动量化版本的 Phi-4 API 服务\n",
    "docker exec -d llamafactory-phi4 llamafactory-cli api \\\n",
    "    --model_name_or_path microsoft/Phi-4 \\\n",
    "    --template default \\\n",
    "    --quantization_bit 4 \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8000\n",
    "\n",
    "echo \"量化版 Phi-4 API 服务已启动在端口 8000\"\n",
    "echo \"使用 4-bit 量化以减少内存使用\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f17a0f",
   "metadata": {},
   "source": [
    "### 3. 监控和日志"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c83a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 监控容器资源使用情况\n",
    "!docker stats llamafactory-phi4\n",
    "\n",
    "# 查看详细的容器信息\n",
    "!docker inspect llamafactory-phi4\n",
    "\n",
    "# 实时查看日志\n",
    "!docker logs -f llamafactory-phi4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00e891d",
   "metadata": {},
   "source": [
    "## 常见问题\n",
    "\n",
    "### 1. Docker 容器内存不足"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61ea528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增加 Docker 容器的内存限制\n",
    "!docker update --memory=32g --memory-swap=64g llamafactory-phi4\n",
    "\n",
    "# 或者在 docker-compose.yml 中设置\n",
    "# deploy:\n",
    "#   resources:\n",
    "#     limits:\n",
    "#       memory: 32G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb96c394",
   "metadata": {},
   "source": [
    "### 2. GPU 在容器中不可用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e6199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查 NVIDIA Container Toolkit 是否正确安装\n",
    "!docker run --rm --gpus all nvidia/cuda:11.8-base-ubuntu22.04 nvidia-smi\n",
    "\n",
    "# 重新配置 Docker 以支持 GPU\n",
    "!sudo nvidia-ctk runtime configure --runtime=docker\n",
    "!sudo systemctl restart docker\n",
    "\n",
    "# 确保 docker-compose.yml 中包含 GPU 配置\n",
    "# deploy:\n",
    "#   resources:\n",
    "#     reservations:\n",
    "#       devices:\n",
    "#         - driver: nvidia\n",
    "#           count: all\n",
    "#           capabilities: [gpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb9451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在容器内设置 Hugging Face 镜像\n",
    "!docker exec llamafactory-phi4 bash -c \"export HF_ENDPOINT=https://hf-mirror.com\"\n",
    "\n",
    "# 或者在 docker-compose.yml 中设置环境变量\n",
    "# environment:\n",
    "#   - HF_ENDPOINT=https://hf-mirror.com\n",
    "\n",
    "# 使用代理下载（如果需要）\n",
    "# environment:\n",
    "#   - https_proxy=http://your-proxy:port\n",
    "#   - http_proxy=http://your-proxy:port"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41784616",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本教程详细介绍了如何在 VM 上使用 LLaMA-Factory Docker 方式部署 Phi-4 模型：\n",
    "\n",
    "### 主要步骤：\n",
    "1. **环境准备**：安装 Docker 和 NVIDIA Container Toolkit\n",
    "2. **项目配置**：克隆 LLaMA-Factory 并配置 Docker Compose\n",
    "3. **模型部署**：构建镜像、启动容器、下载模型\n",
    "4. **服务启动**：启动 API 服务和 Web UI\n",
    "5. **测试验证**：通过 API 和客户端测试模型功能\n",
    "\n",
    "### 优势：\n",
    "- **环境隔离**：Docker 容器提供干净的运行环境\n",
    "- **易于管理**：通过 Docker Compose 统一管理服务\n",
    "- **可扩展性**：支持 GPU 加速和资源优化\n",
    "- **便于部署**：一键启动所有服务\n",
    "\n",
    "### 注意事项：\n",
    "- 确保 VM 有足够的内存和存储空间\n",
    "- 正确配置 GPU 支持（如果使用）\n",
    "- 合理设置资源限制和优化参数\n",
    "- 定期监控容器状态和资源使用情况"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
