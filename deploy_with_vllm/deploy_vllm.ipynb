{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vLLM\n",
    "\n",
    "vLLM 是一个高吞吐量且内存高效的推理和服务引擎，专为大型语言模型（LLM）设计。它通过利用先进的内存管理技术，如 PagedAttention，有效管理注意力键和值的内存，从而优化 LLM 的服务和执行。这使得能够对传入请求进行连续批处理并实现快速模型执行，使 vLLM 成为大规模部署和服务 LLM 的强大工具。\n",
    "\n",
    "vLLM 支持与流行的 Hugging Face 模型无缝集成，并提供多种解码算法，包括并行采样和束搜索。它还支持张量并行和流水线并行以实现分布式推理，使其成为一个灵活且易于使用的 LLM 推理解决方案（详见完整文档）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前置条件\n",
    "\n",
    "配置当前订阅，默认的工作区和资源组，如下图所示，从右上角菜单查看\n",
    "\n",
    "- 工作区: 工作区是与同事协作创建机器学习工件和分组相关工作的场所。例如，实验、作业、数据集、模型、组件和推理端点。\n",
    "- 资源组: Azure 中的资源组是一个逻辑容器, 它允许您根据资源的生命周期和安全需求将这些资源作为一个整体进行管理。资源组中的资源可以包括虚拟机、存储账户和虚拟网络等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![描述文本](./setup.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_group=\"test-ml-rg2\"\n",
    "subscription=\"b1d92895-527c-4a67-91d2-8f653d9ee248\"\n",
    "workspace=\"test-ml-ws\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行配置命令"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az account set --subscription {subscription}\n",
    "!az configure --defaults workspace={workspace} group={resource_group}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一步，在 AzureML 上为 vLLM 创建自定义环境\n",
    "\n",
    "环境（Environment）是 Azure 机器学习中的一个概念，它定义了用于训练和推理的计算资源。环境可以包含多个组件，例如 Python 包、依赖项和配置。\n",
    "\n",
    "这里通过 `environment.yml` 文件指定环境配置：更复杂的配置参考 [CLI (v2) environment YAML schema](https://learn.microsoft.com/en-us/azure/machine-learning/reference-yaml-environment?view=azureml-api-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/environment.schema.json\n",
    "name: vllm\n",
    "build:\n",
    "  path: .\n",
    "  dockerfile_path: Dockerfile\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. $schema: …environment.schema.json\n",
    "- 指定了这个 YAML 文件所遵循的 JSON Schema 地址。\n",
    "- AzureML 在解析时会根据这个 schema 对文件内容做校验，确保字段和格式都符合最新的环境定义规范。\n",
    "2. name: vllm\n",
    "- 定义了在 AzureML 中注册的环境名称，这里你把它命名为 vllm。\n",
    "- 后续在创建计算集群或推理部署时，就可以通过这个名称来引用刚刚定义好的环境。\n",
    "3. build:\n",
    "这个部分告诉 AzureML 如何构建底层的 Docker 镜像。\n",
    "- path: .\n",
    "  - 构建上下文（build context）的目录路径。\n",
    "  - . 表示当前文件所在目录，所有的依赖文件（如 Dockerfile、代码、依赖列表等）都应该放在这里或其子目录中。\n",
    "- dockerfile_path: Dockerfile\n",
    "  - 指定要使用的 Dockerfile 文件名或路径。\n",
    "\n",
    "执行命令构建环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az ml environment create -f environment.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二步，创建 AzureML 托管的在线端点\n",
    "\n",
    "接下来开始正式部署, 首先创建托管在线端点\n",
    "\n",
    "托管在线端点（Managed Online Endpoint）是 Azure 机器学习中的一个概念，它允许您在 Azure 机器学习平台上部署和管理在线推理服务。托管在线端点提供了一种简化的方式来部署和管理模型，并支持自动缩放和监控。\n",
    "\n",
    "![](./online-endpoint.png)\n",
    "\n",
    "创建 `endpoint.yml` 文件，更多配置参考 [CLI (v2) managed online endpoint YAML schema](https://learn.microsoft.com/en-us/azure/machine-learning/reference-yaml-endpoint-online?view=azureml-api-2)\n",
    "\n",
    "```yml\n",
    "$schema: https://azuremlsdk2.blob.core.windows.net/latest/managedOnlineEndpoint.schema.json\n",
    "name: vllm-hf\n",
    "auth_mode: key\n",
    "```\n",
    "\n",
    "1. $schema: …managedOnlineEndpoint.schema.json\n",
    "  - 指定了该 YAML 文件所遵循的 JSON Schema。\n",
    "  - Azure ML 在解析此文件时，会根据这个 Schema 验证字段名称、类型和结构是否正确。\n",
    "  - 这里指向的是最新版本的 “托管在线端点” 定义，确保你使用的是最新的字段和语法规范。\n",
    "2. name: vllm-hf\n",
    "  - 为这个托管的在线端点（Endpoint）设置一个唯一名称，便于在同一个 Azure ML 工作区中进行管理和调用。\n",
    "  - 该名称会用于 CLI、SDK 或 REST API 中引用该端点，比如后续创建部署或发送推理请求时用到。\n",
    "3. auth_mode: key\n",
    "  - 指定该端点的认证方式为 “Key” 模式。\n",
    "  - 意味着访问这个端点时，需要在请求头中带上由 Azure ML 生成的访问密钥（Primary/Secondary Key）。\n",
    "  - 其他可选模式还包括 aad_token, aml_token 等，但这里使用最简单的密钥认证。\n",
    "\n",
    "执行命令创建部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az ml online-endpoint create -f endpoint.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来的步骤我们需要查看一下刚刚创建的 Docker 镜像地址\n",
    "可以从 AzureML Studio -> Environments -> vlllm: 看到\n",
    "\n",
    "![](./arc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们创建 `deployment.yml` 文件配置部署相关设置，并且部署模型\n",
    "\n",
    "```yml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json\n",
    "name: current\n",
    "endpoint_name: vllm-hf\n",
    "environment_variables:\n",
    "  MODEL_NAME: meta-llama/Llama-3.1-8B-Instruct # define the model name using the identifier from HG\n",
    "  VLLM_ARGS: \"--enable-auto-tool-choice --tool-call-parser llama3_json\"\n",
    "  HUGGING_FACE_HUB_TOKEN: xxxxxxxxxxxxxx # Add your HF API key here\n",
    "environment:\n",
    "  image: xxxxxxxxx.azurecr.io/azureml/azureml_xxxxxxxxxxx # Replace with your own image\n",
    "  inference_config:\n",
    "    liveness_route:\n",
    "      port: 8000\n",
    "      path: /ping \n",
    "    readiness_route:\n",
    "      port: 8000\n",
    "      path: /health\n",
    "    scoring_route:\n",
    "      port: 8000\n",
    "      path: /\n",
    "instance_type: Standard_NC24ads_A100_v4\n",
    "instance_count: 1\n",
    "request_settings:\n",
    "    request_timeout_ms: 60000\n",
    "    max_concurrent_requests_per_instance: 16 \n",
    "liveness_probe:\n",
    "  initial_delay: 10\n",
    "  period: 10\n",
    "  timeout: 2\n",
    "  success_threshold: 1\n",
    "  failure_threshold: 30\n",
    "readiness_probe:\n",
    "  initial_delay: 120\n",
    "  period: 10\n",
    "  timeout: 2\n",
    "  success_threshold: 1\n",
    "  failure_threshold: 30\n",
    "```\n",
    "\n",
    "由于 vLLM 不支持分别用于就绪性和存活性的探针，我们需要确保模型完全加载后再触发第一个探针。这就是为什么我们将 readiness_probe.initial_delay 增加到 120 秒。\n",
    "\n",
    "对于更大的模型，我们还应按照 vLLM 的文档使用张量并行推理（模型在单个节点上但跨多个 GPU）的方法，通过向 VLLM_ARGS 添加--tensor-parallel-size <GPU 数量>。由于我们示例中使用的是单个 A100 GPU（Standard_NC24ads_A100_v4），因此这不是必需的。\n",
    "\n",
    "request_settings 在一定程度上取决于我们的实例类型/大小，可能需要一些手动调优以使模型运行得正确且高效。目标是在并发量（max_concurrent_requests_per_instance）和排队时间之间找到一个良好的平衡，以避免端点侧触发 request_timeout_ms，或客户端侧出现任何 HTTP 超时。\n",
    "\n",
    "这两种情况都会导致 HTTP 429，客户端需要实现指数退避（例如通过 tenacity 库）。\n",
    "\n",
    "最后，我们可以执行命令部署模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m(UserError) A deployment with this name already exists. If you are trying to create a new deployment, use a\n",
      "different name. If you are trying to update an existing deployment, use `az ml online-deployment update` instead.\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!az ml online-deployment create -f deployment.yml --all-traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果已经创建，执行更新操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: endpoint vllm-hf exists\n",
      "\u001b[K\u001b[K\u001b[91m(ResourceNotReady) User container has crashed or terminated. Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-resourcenotready Running ..\u001b[K - Running ..\u001b[K / Running ..\u001b[K \\ Running ..\u001b[K \\ Running ..\u001b[K / Running ..\u001b[K | Running ..\u001b[K \\ Running ..\u001b[K - Running ..\u001b[K / Running ..\u001b[K | Running ..\u001b[K - Running ..\u001b[K / Running ..\u001b[K \\ Running ..\u001b[K - Running ..\u001b[K / Running ..\u001b[K | Running ..\u001b[K \\ Running ..\u001b[K - Interrupted ..\n",
      "Code: ResourceNotReady\n",
      "Message: User container has crashed or terminated. Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-resourcenotready\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!az ml online-deployment update -f deployment.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果提示 \"Specified deployment [current] failed during initial provisioning and is in an unrecoverable state. Delete and re-create\" \n",
    "执行删除操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K / Finished ..\u001b[0m"
     ]
    }
   ],
   "source": [
    "!az ml online-deployment delete -n current --endpoint-name vllm-hf --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第 3 步：测试部署\n",
    "\n",
    "首先我们获取端点的评分 URI 和 API 密钥:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az ml online-endpoint show -n vllm-hf\n",
    "!az ml online-endpoint get-credentials -n vllm-hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于补全模型，我们可以使用以下 Python 代码片段调用该端点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://vllm-hf.polandcentral.inference.ml.azure.com/v1/completions\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer xxxxxxxxxxxx\"\n",
    "}\n",
    "data = {\n",
    "    \"model\": \"openai-community/gpt2\",\n",
    "    \"prompt\": \"San Francisco is a\",\n",
    "    \"max_tokens\": 200,\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结论\n",
    "\n",
    "总之，在 Azure 机器学习托管在线端点上使用 vLLM 部署大型语言模型是一种简单有效的方式，可以实现实时的 AI 驱动应用。通过遵循所分享的步骤——从环境设置到部署测试——您可以快速将 Llama-3.1-8B-Instruct 等先进模型集成到工作流程中。借助 vLLM 的优化性能和对函数调用的支持，您的应用能够无缝处理复杂任务并与其他系统交互。此配置帮助您构建更智能、更快速且更具可扩展性的 AI 解决方案。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshoot\n",
    "\n",
    "### 解决存储账户网络访问权限问题\n",
    "\n",
    "配置允许公网访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable public network access for Azure storage account using az cli\n",
    "!az storage account update \\\n",
    "    --name $(az storage account list -g {resource_group} --query \"[0].name\" -o tsv) \\\n",
    "    --resource-group {resource_group} \\\n",
    "    --public-network-access Enabled \\\n",
    "    --default-action Allow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "配置允许存储账户密钥访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable storage account key access using az cli\n",
    "!az storage account update \\\n",
    "    --name $(az storage account list -g {resource_group} --query \"[0].name\" -o tsv) \\\n",
    "    --resource-group {resource_group} \\\n",
    "    --allow-shared-key-access true"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
