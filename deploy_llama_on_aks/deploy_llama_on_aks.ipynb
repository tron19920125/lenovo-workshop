{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2de07432",
   "metadata": {},
   "source": [
    "# 在 Azure Kubernetes Service (AKS) 上使用 LLaMA-Factory Docker 部署微调模型\n",
    "\n",
    "本教程将指导您使用 LLaMA-Factory Docker 镜像在 AKS 上部署微调后的模型，实现一个可扩展的模型推理服务。\n",
    "\n",
    "## 目录\n",
    "1. [前置条件](#前置条件)\n",
    "2. [环境准备](#环境准备)\n",
    "3. [创建 AKS 集群](#创建-aks-集群)\n",
    "4. [配置 Container Registry](#配置-container-registry)\n",
    "5. [创建 LLaMA-Factory 部署文件](#创建-llamafactory-部署文件)\n",
    "6. [部署到 AKS](#部署到-aks)\n",
    "7. [配置负载均衡器](#配置负载均衡器)\n",
    "8. [测试模型端点](#测试模型端点)\n",
    "9. [监控和扩展](#监控和扩展)\n",
    "10. [清理资源](#清理资源)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a522521",
   "metadata": {},
   "source": [
    "## 前置条件\n",
    "\n",
    "### 系统要求\n",
    "- Azure 订阅（如果没有，请创建[免费账户](https://azure.microsoft.com/free/)）\n",
    "- Azure CLI 2.50.0 或更高版本\n",
    "- kubectl 命令行工具\n",
    "- Docker 命令行工具\n",
    "- 至少 16GB RAM 的开发机器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d472c2",
   "metadata": {},
   "source": [
    "### 安装必要工具\n",
    "\n",
    "首先，我们需要安装和配置必要的工具。以下命令将安装 Azure CLI、kubectl 和相关扩展："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa507d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装 Azure CLI (如果尚未安装)\n",
    "!curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n",
    "\n",
    "# 验证安装\n",
    "!az --version\n",
    "\n",
    "# 安装 kubectl\n",
    "!az aks install-cli\n",
    "\n",
    "# 安装 k8s-extension 扩展\n",
    "!az extension add --name k8s-extension\n",
    "\n",
    "# 更新扩展到最新版本\n",
    "!az extension update --name k8s-extension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64672fcb",
   "metadata": {},
   "source": [
    "### 登录 Azure\n",
    "\n",
    "使用以下命令登录到 Azure 并设置默认订阅："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3059ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# 设置环境变量\n",
    "os.environ[\"RESOURCE_GROUP\"] = \"llama-aks-rg\"\n",
    "os.environ[\"LOCATION\"] = \"eastus\"\n",
    "os.environ[\"AKS_CLUSTER_NAME\"] = \"llama-aks-cluster\"\n",
    "os.environ[\"ACR_NAME\"] = f\"llamaacr{random.randint(0, 999999)}\"\n",
    "os.environ[\"NAMESPACE\"] = \"llama-inference\"\n",
    "os.environ[\"SERVICE_NAME\"] = \"llama-service\"\n",
    "os.environ[\"DEPLOYMENT_NAME\"] = \"llama-deployment\"\n",
    "\n",
    "# 显示配置\n",
    "print(f\"Resource Group: {os.environ['RESOURCE_GROUP']}\")\n",
    "print(f\"Location: {os.environ['LOCATION']}\")\n",
    "print(f\"AKS Cluster: {os.environ['AKS_CLUSTER_NAME']}\")\n",
    "print(f\"ACR Name: {os.environ['ACR_NAME']}\")\n",
    "print(f\"Namespace: {os.environ['NAMESPACE']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d07c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# 设置环境变量\n",
    "os.environ[\"RESOURCE_GROUP\"] = \"phi4-aks-rg\"\n",
    "os.environ[\"LOCATION\"] = \"eastus\"\n",
    "os.environ[\"AKS_CLUSTER_NAME\"] = \"phi4-aks-cluster\"\n",
    "os.environ[\"ML_WORKSPACE_NAME\"] = \"phi4-ml-workspace\"\n",
    "os.environ[\"EXTENSION_NAME\"] = \"phi4-ml-extension\"\n",
    "os.environ[\"COMPUTE_NAME\"] = \"phi4-k8s-compute\"\n",
    "os.environ[\"ENDPOINT_NAME\"] = \"phi4-endpoint\"\n",
    "os.environ[\"DEPLOYMENT_NAME\"] = \"phi4-deployment\"\n",
    "os.environ[\"ACR_NAME\"] = f\"phi4acr{random.randint(0, 999999)}\"\n",
    "os.environ[\"STORAGE_ACCOUNT\"] = f\"phi4storage{random.randint(0, 999999)}\"\n",
    "\n",
    "# 显示配置\n",
    "print(f\"Resource Group: {os.environ['RESOURCE_GROUP']}\")\n",
    "print(f\"Location: {os.environ['LOCATION']}\")\n",
    "print(f\"AKS Cluster: {os.environ['AKS_CLUSTER_NAME']}\")\n",
    "print(f\"ML Workspace: {os.environ['ML_WORKSPACE_NAME']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20e0709",
   "metadata": {},
   "source": [
    "## 创建 AKS 集群\n",
    "\n",
    "### 1. 创建资源组\n",
    "\n",
    "首先，我们需要创建一个资源组来管理所有相关资源："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e366ec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建资源组\n",
    "!az group create \\\n",
    "    --name $RESOURCE_GROUP \\\n",
    "    --location $LOCATION\n",
    "\n",
    "# 验证资源组创建\n",
    "!az group show --name $RESOURCE_GROUP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a816f4f",
   "metadata": {},
   "source": [
    "### 2. 创建 AKS 集群\n",
    "\n",
    "创建一个适合运行 LLaMA 模型的 AKS 集群："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bc54fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!az aks create \\\n",
    "    --resource-group $RESOURCE_GROUP \\\n",
    "    --name $AKS_CLUSTER_NAME \\\n",
    "    --node-count 2 \\\n",
    "    --node-vm-size Standard_NC6s_v3 \\\n",
    "    --enable-managed-identity \\\n",
    "    --generate-ssh-keys \\\n",
    "    --enable-addons monitoring \\\n",
    "    --enable-cluster-autoscaler \\\n",
    "    --min-count 1 \\\n",
    "    --max-count 3 \\\n",
    "    --node-osdisk-size 100\n",
    "\n",
    "# 获取 AKS 凭据\n",
    "!az aks get-credentials \\\n",
    "    --resource-group $RESOURCE_GROUP \\\n",
    "    --name $AKS_CLUSTER_NAME \\\n",
    "    --overwrite-existing\n",
    "\n",
    "# 验证集群连接\n",
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9890b7ab",
   "metadata": {},
   "source": [
    "### 3. 安装 GPU 驱动程序（如果使用 GPU 节点）\n",
    "\n",
    "为 AKS 集群配置 GPU 支持："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8fbc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装 NVIDIA GPU 设备插件\n",
    "!kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.14.1/nvidia-device-plugin.yml\n",
    "\n",
    "# 验证 GPU 节点\n",
    "!kubectl get nodes -o wide\n",
    "\n",
    "# 检查 GPU 资源\n",
    "!kubectl describe nodes | grep -A 5 \"Capacity\\|Allocatable\" | grep nvidia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4740db",
   "metadata": {},
   "source": [
    "## 配置 Container Registry\n",
    "\n",
    "### 1. 创建 Azure Container Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2534e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 Azure Container Registry\n",
    "!az acr create \\\n",
    "    --name $ACR_NAME \\\n",
    "    --resource-group $RESOURCE_GROUP \\\n",
    "    --location $LOCATION \\\n",
    "    --sku Standard \\\n",
    "    --admin-enabled true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaed477",
   "metadata": {},
   "source": [
    "登录到ACR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8e62ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!az acr login --name $ACR_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa101b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "# 获取 ACR 登录服务器\n",
    "ACR_LOGIN_SERVER=$(az acr show --name $ACR_NAME --resource-group $RESOURCE_GROUP --query loginServer -o tsv)\n",
    "echo \"ACR Login Server: $ACR_LOGIN_SERVER\"\n",
    "\n",
    "# 将 ACR 与 AKS 集群关联\n",
    "!az aks update \\\n",
    "    --name $AKS_CLUSTER_NAME \\\n",
    "    --resource-group $RESOURCE_GROUP \\\n",
    "    --attach-acr $ACR_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9635c55e",
   "metadata": {},
   "source": [
    "### 2. 准备 LLaMA-Factory Docker 镜像\n",
    "\n",
    "我们可以直接使用官方的 LLaMA-Factory 镜像，或者自定义镜像："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a3cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 获取 ACR 登录服务器地址\n",
    "ACR_LOGIN_SERVER=$(az acr show --name $ACR_NAME --resource-group $RESOURCE_GROUP --query loginServer -o tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56f4a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拉取官方 LLaMA-Factory 镜像\n",
    "!docker pull registry.cn-hangzhou.aliyuncs.com/modelscope-repo/llamafactory:latest\n",
    "\n",
    "# 标记镜像\n",
    "!docker tag registry.cn-hangzhou.aliyuncs.com/modelscope-repo/llamafactory:latest ${ACR_LOGIN_SERVER}/llamafactory:latest\n",
    "\n",
    "# 推送到 ACR\n",
    "!docker push ${ACR_LOGIN_SERVER}/llamafactory:latest\n",
    "\n",
    "# 验证镜像推送\n",
    "!az acr repository list --name $ACR_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5918955b",
   "metadata": {},
   "source": [
    "## 创建 LLaMA-Factory 部署文件\n",
    "\n",
    "### 1. 创建 Namespace 和 ConfigMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b292b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建命名空间\n",
    "!kubectl create namespace $NAMESPACE\n",
    "\n",
    "# 创建配置文件目录\n",
    "!mkdir -p k8s-manifests\n",
    "\n",
    "# 验证命名空间创建\n",
    "!kubectl get namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ea909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile k8s-manifests/configmap.yaml\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: llama-config\n",
    "  namespace: llama-inference\n",
    "data:\n",
    "  API_HOST: \"0.0.0.0\"\n",
    "  API_PORT: \"8000\"\n",
    "  WEBUI_HOST: \"0.0.0.0\" \n",
    "  WEBUI_PORT: \"7860\"\n",
    "  MODEL_NAME: \"your-model-name\"  # 替换为您的模型名称\n",
    "  ADAPTER_NAME: \"your-adapter-path\"  # 如果有微调适配器的话\n",
    "  QUANTIZATION_BIT: \"4\"\n",
    "  TEMPLATE: \"default\"\n",
    "  HF_ENDPOINT: \"https://hf-mirror.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d647a138",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile k8s-manifests/deployment.yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: phi4-deployment\n",
    "  namespace: phi4-inference\n",
    "  labels:\n",
    "    app: phi4\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: phi4\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: phi4\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: llamafactory\n",
    "        image: ${ACR_LOGIN_SERVER}/llamafactory:latest\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "          name: api\n",
    "        - containerPort: 7860\n",
    "          name: webui\n",
    "        env:\n",
    "        - name: CUDA_VISIBLE_DEVICES\n",
    "          value: \"0\"\n",
    "        - name: HF_HOME\n",
    "          value: \"/app/models\"\n",
    "        - name: HF_HUB_CACHE\n",
    "          value: \"/app/models\"\n",
    "        envFrom:\n",
    "        - configMapRef:\n",
    "            name: phi4-config\n",
    "        command: [\"/bin/bash\"]\n",
    "        args:\n",
    "        - -c\n",
    "        - |\n",
    "          # 启动 API 服务\n",
    "          llamafactory-cli api \\\n",
    "            --model_name_or_path $(MODEL_NAME) \\\n",
    "            --template $(TEMPLATE) \\\n",
    "            --quantization_bit $(QUANTIZATION_BIT) \\\n",
    "            --host $(API_HOST) \\\n",
    "            --port $(API_PORT) &\n",
    "          \n",
    "          # 启动 Web UI\n",
    "          llamafactory-cli webui \\\n",
    "            --host $(WEBUI_HOST) \\\n",
    "            --port $(WEBUI_PORT) &\n",
    "          \n",
    "          # 保持容器运行\n",
    "          wait\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"8Gi\"\n",
    "            cpu: \"2\"\n",
    "            nvidia.com/gpu: 1\n",
    "          limits:\n",
    "            memory: \"16Gi\"\n",
    "            cpu: \"4\"\n",
    "            nvidia.com/gpu: 1\n",
    "        volumeMounts:\n",
    "        - name: model-cache\n",
    "          mountPath: /app/models\n",
    "        - name: shm\n",
    "          mountPath: /dev/shm\n",
    "        readinessProbe:\n",
    "          httpGet:\n",
    "            path: /docs\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 300\n",
    "          periodSeconds: 30\n",
    "          timeoutSeconds: 10\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /docs\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 600\n",
    "          periodSeconds: 60\n",
    "          timeoutSeconds: 30\n",
    "      volumes:\n",
    "      - name: model-cache\n",
    "        emptyDir:\n",
    "          sizeLimit: 50Gi\n",
    "      - name: shm\n",
    "        emptyDir:\n",
    "          medium: Memory\n",
    "          sizeLimit: 8Gi\n",
    "      tolerations:\n",
    "      - key: \"sku\"\n",
    "        operator: \"Equal\"\n",
    "        value: \"gpu\"\n",
    "        effect: \"NoSchedule\"\n",
    "      nodeSelector:\n",
    "        accelerator: nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fba8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile k8s-manifests/service.yaml\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: phi4-service\n",
    "  namespace: phi4-inference\n",
    "  labels:\n",
    "    app: phi4\n",
    "spec:\n",
    "  type: LoadBalancer\n",
    "  ports:\n",
    "  - name: api\n",
    "    port: 8000\n",
    "    targetPort: 8000\n",
    "    protocol: TCP\n",
    "  - name: webui\n",
    "    port: 7860\n",
    "    targetPort: 7860\n",
    "    protocol: TCP\n",
    "  selector:\n",
    "    app: phi4\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: phi4-api-service\n",
    "  namespace: phi4-inference  \n",
    "  labels:\n",
    "    app: phi4\n",
    "spec:\n",
    "  type: ClusterIP\n",
    "  ports:\n",
    "  - name: api\n",
    "    port: 8000\n",
    "    targetPort: 8000\n",
    "    protocol: TCP\n",
    "  selector:\n",
    "    app: phi4\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: phi4-webui-service\n",
    "  namespace: phi4-inference\n",
    "  labels:\n",
    "    app: phi4\n",
    "spec:\n",
    "  type: ClusterIP\n",
    "  ports:\n",
    "  - name: webui\n",
    "    port: 7860\n",
    "    targetPort: 7860\n",
    "    protocol: TCP\n",
    "  selector:\n",
    "    app: phi4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e23610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile k8s-manifests/service.yaml\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: llama-service\n",
    "  namespace: llama-inference\n",
    "  labels:\n",
    "    app: llama\n",
    "spec:\n",
    "  type: LoadBalancer\n",
    "  ports:\n",
    "  - name: api\n",
    "    port: 8000\n",
    "    targetPort: 8000\n",
    "    protocol: TCP\n",
    "  - name: webui\n",
    "    port: 7860\n",
    "    targetPort: 7860\n",
    "    protocol: TCP\n",
    "  selector:\n",
    "    app: llama\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: llama-api-service\n",
    "  namespace: llama-inference  \n",
    "  labels:\n",
    "    app: llama\n",
    "spec:\n",
    "  type: ClusterIP\n",
    "  ports:\n",
    "  - name: api\n",
    "    port: 8000\n",
    "    targetPort: 8000\n",
    "    protocol: TCP\n",
    "  selector:\n",
    "    app: llama\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: llama-webui-service\n",
    "  namespace: llama-inference\n",
    "  labels:\n",
    "    app: llama\n",
    "spec:\n",
    "  type: ClusterIP\n",
    "  ports:\n",
    "  - name: webui\n",
    "    port: 7860\n",
    "    targetPort: 7860\n",
    "    protocol: TCP\n",
    "  selector:\n",
    "    app: llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b7f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 获取 ACR 登录服务器地址\n",
    "ACR_LOGIN_SERVER=$(az acr show --name $ACR_NAME --resource-group $RESOURCE_GROUP --query loginServer -o tsv)\n",
    "\n",
    "# 替换部署文件中的镜像地址\n",
    "!sed -i \"s/\\${ACR_LOGIN_SERVER}/$ACR_LOGIN_SERVER/g\" k8s-manifests/deployment.yaml\n",
    "\n",
    "# 应用所有配置文件\n",
    "!kubectl apply -f k8s-manifests/\n",
    "\n",
    "# 检查部署状态\n",
    "!kubectl get pods -n $NAMESPACE\n",
    "!kubectl get services -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2305c53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 等待 Pod 就绪\n",
    "!kubectl wait --for=condition=ready pod -l app=phi4 -n $NAMESPACE --timeout=600s\n",
    "\n",
    "# 查看详细状态\n",
    "!kubectl describe pods -l app=phi4 -n $NAMESPACE\n",
    "\n",
    "# 查看日志\n",
    "!kubectl logs -l app=phi4 -n $NAMESPACE --tail=50\n",
    "\n",
    "# 检查服务状态\n",
    "!kubectl get svc -n $NAMESPACE -o wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6747cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 等待外部 IP 分配\n",
    "!kubectl get service phi4-service -n $NAMESPACE --watch\n",
    "\n",
    "# 获取外部 IP（这可能需要几分钟时间）\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "def get_external_ip():\n",
    "    for _ in range(60):  # 等待最多10分钟\n",
    "        result = subprocess.run([\n",
    "            'kubectl', 'get', 'service', 'phi4-service', \n",
    "            '-n', os.environ['NAMESPACE'],\n",
    "            '-o', 'jsonpath={.status.loadBalancer.ingress[0].ip}'\n",
    "        ], capture_output=True, text=True)\n",
    "        \n",
    "        if result.stdout and result.stdout.strip():\n",
    "            return result.stdout.strip()\n",
    "        \n",
    "        print(\"等待外部 IP 分配...\")\n",
    "        time.sleep(10)\n",
    "    \n",
    "    return None\n",
    "\n",
    "external_ip = get_external_ip()\n",
    "if external_ip:\n",
    "    print(f\"外部 IP 地址: {external_ip}\")\n",
    "    print(f\"API 端点: http://{external_ip}:8000\")\n",
    "    print(f\"Web UI: http://{external_ip}:7860\")\n",
    "else:\n",
    "    print(\"未能获取外部 IP 地址，请手动检查\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d58fb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试 API 健康状态\n",
    "!curl -X GET http://{external_ip}:8000/v1/models\n",
    "\n",
    "# 测试文档页面\n",
    "!curl -X GET http://{external_ip}:8000/docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3351696",
   "metadata": {},
   "source": [
    "### 2. 使用 Python 测试 API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a99ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# API 端点\n",
    "api_url = f\"http://{external_ip}:8000/v1/chat/completions\"\n",
    "\n",
    "# 测试数据\n",
    "test_cases = [\n",
    "    {\n",
    "        \"model\": \"microsoft/Phi-4\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"解释什么是量子计算？\"}\n",
    "        ],\n",
    "        \"max_tokens\": 200,\n",
    "        \"temperature\": 0.7\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"microsoft/Phi-4\", \n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"写一个 Python 函数计算斐波那契数列\"}\n",
    "        ],\n",
    "        \"max_tokens\": 300,\n",
    "        \"temperature\": 0.5\n",
    "    }\n",
    "]\n",
    "\n",
    "# 发送测试请求\n",
    "for i, test_data in enumerate(test_cases, 1):\n",
    "    print(f\"\\n=== 测试 {i} ===\")\n",
    "    print(f\"问题: {test_data['messages'][0]['content']}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            api_url,\n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "            json=test_data,\n",
    "            timeout=120\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            content = result.get('choices', [{}])[0].get('message', {}).get('content', '')\n",
    "            usage = result.get('usage', {})\n",
    "            \n",
    "            print(f\"回答: {content}\")\n",
    "            print(f\"令牌使用: {usage}\")\n",
    "        else:\n",
    "            print(f\"请求失败: {response.status_code}\")\n",
    "            print(f\"错误信息: {response.text}\")\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"请求异常: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"其他错误: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff568c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看 Pod 资源使用情况\n",
    "!kubectl top pods -n $NAMESPACE\n",
    "\n",
    "# 查看节点资源使用情况  \n",
    "!kubectl top nodes\n",
    "\n",
    "# 查看 Pod 事件\n",
    "!kubectl get events -n $NAMESPACE --sort-by='.lastTimestamp'\n",
    "\n",
    "# 实时监控日志\n",
    "!kubectl logs -f -l app=phi4 -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5063ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手动扩展 Deployment\n",
    "!kubectl scale deployment phi4-deployment --replicas=2 -n $NAMESPACE\n",
    "\n",
    "# 查看扩展状态\n",
    "!kubectl get pods -n $NAMESPACE -l app=phi4\n",
    "\n",
    "# 设置 Horizontal Pod Autoscaler (HPA)\n",
    "!kubectl autoscale deployment phi4-deployment \\\n",
    "    --cpu-percent=70 \\\n",
    "    --min=1 \\\n",
    "    --max=3 \\\n",
    "    -n $NAMESPACE\n",
    "\n",
    "# 查看 HPA 状态\n",
    "!kubectl get hpa -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6f2efc",
   "metadata": {},
   "source": [
    "## 清理资源\n",
    "\n",
    "当您完成测试后，记得清理创建的资源以避免不必要的费用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3af8ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除部署\n",
    "!kubectl delete -f k8s-manifests/\n",
    "\n",
    "# 删除命名空间\n",
    "!kubectl delete namespace $NAMESPACE\n",
    "\n",
    "# 删除 ACR\n",
    "!az acr delete \\\n",
    "    --name $ACR_NAME \\\n",
    "    --resource-group $RESOURCE_GROUP \\\n",
    "    --yes\n",
    "\n",
    "# 删除 AKS 集群\n",
    "!az aks delete \\\n",
    "    --name $AKS_CLUSTER_NAME \\\n",
    "    --resource-group $RESOURCE_GROUP \\\n",
    "    --yes\n",
    "\n",
    "# 删除资源组（这将删除所有剩余资源）\n",
    "!az group delete \\\n",
    "    --name $RESOURCE_GROUP \\\n",
    "    --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409d2936",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本教程详细介绍了如何在 AKS 上使用 LLaMA-Factory Docker 方式部署微调后的模型：\n",
    "\n",
    "### 主要步骤：\n",
    "1. **环境准备**：创建 AKS 集群和 Azure Container Registry\n",
    "2. **GPU 配置**：安装 NVIDIA GPU 设备插件\n",
    "3. **镜像管理**：拉取并推送 LLaMA-Factory 镜像到 ACR\n",
    "4. **Kubernetes 配置**：创建 ConfigMap、Deployment 和 Service 配置文件\n",
    "5. **服务部署**：部署到 AKS 并配置负载均衡器\n",
    "6. **测试验证**：通过 API 和 Web UI 测试模型功能\n",
    "7. **监控扩展**：监控资源使用情况并配置自动扩展\n",
    "\n",
    "### 优势：\n",
    "- **云原生架构**：利用 Kubernetes 的编排和管理能力\n",
    "- **高可用性**：支持多副本部署和负载均衡\n",
    "- **弹性扩展**：自动扩展以应对负载变化\n",
    "- **资源隔离**：通过命名空间和资源限制确保隔离\n",
    "- **易于管理**：统一的部署和监控方式\n",
    "- **成本效益**：不需要 Azure ML 的额外费用\n",
    "\n",
    "### 注意事项：\n",
    "- 确保 AKS 集群有足够的 GPU 资源\n",
    "- 合理设置资源请求和限制\n",
    "- 监控模型下载和启动时间\n",
    "- 配置适当的健康检查探针\n",
    "- 定期备份重要的配置文件\n",
    "- 根据实际需求调整模型名称和适配器路径\n",
    "\n",
    "### 自定义配置：\n",
    "- 在 ConfigMap 中更新 `MODEL_NAME` 为您的实际模型名称\n",
    "- 如果使用微调适配器，请设置 `ADAPTER_NAME` \n",
    "- 根据需要调整量化参数和资源限制\n",
    "- 根据负载情况调整副本数量和 HPA 设置"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
