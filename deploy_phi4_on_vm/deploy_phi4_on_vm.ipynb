{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8693432",
   "metadata": {},
   "source": [
    "# VM 部署 Phi-4 模型完整教程\n",
    "\n",
    "本 Jupyter Notebook 提供了在虚拟机（VM）上部署 Phi-4 模型的完整指南。本教程涵盖了从环境配置到模型部署和测试的所有步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe41467",
   "metadata": {},
   "source": [
    "## 目录\n",
    "1. [概述](#概述)\n",
    "2. [环境要求](#环境要求)\n",
    "3. [VM 环境准备](#vm-环境准备)\n",
    "4. [安装依赖](#安装依赖)\n",
    "5. [下载模型](#下载模型)\n",
    "6. [部署模型](#部署模型)\n",
    "7. [测试验证](#测试验证)\n",
    "8. [性能优化](#性能优化)\n",
    "9. [常见问题](#常见问题)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e7df9c",
   "metadata": {},
   "source": [
    "## 概述\n",
    "\n",
    "Phi-4 是微软发布的一个高效的小型语言模型，本教程将指导您如何在虚拟机（VM）上完整部署 Phi-4 模型，包括环境配置、模型下载、部署和测试等步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bffd092",
   "metadata": {},
   "source": [
    "## 环境要求\n",
    "\n",
    "### 硬件要求\n",
    "- **CPU**: 至少 8 核心（推荐 16 核心或以上）\n",
    "- **内存**: 最少 16GB RAM（推荐 32GB 或以上）\n",
    "- **存储**: 至少 50GB 可用空间\n",
    "- **GPU**: 可选，但强烈推荐使用 NVIDIA GPU（至少 8GB 显存）\n",
    "\n",
    "### 软件要求\n",
    "- **操作系统**: Ubuntu 20.04/22.04 LTS 或 CentOS 7/8\n",
    "- **Python**: 3.8 或更高版本\n",
    "- **CUDA**: 11.8 或更高版本（如果使用 GPU）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af8915f",
   "metadata": {},
   "source": [
    "## 环境准备\n",
    "\n",
    "### 定义环境变量\n",
    "\n",
    "我们需要设置一些环境变量来管理我们的资源："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff4dbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# 设置环境变量\n",
    "os.environ[\"RESOURCE_GROUP\"] = \"phi4-rg\"\n",
    "os.environ[\"LOCATION\"] = \"eastus3\"\n",
    "os.environ['VM_NAME'] = \"phi4-vm\"\n",
    "os.environ['VM_SIZE'] = \"Standard_NC6s_v3\"\n",
    "os.environ[\"COMPUTE_NAME\"] = \"phi4-k8s-compute\"\n",
    "os.environ[\"STORAGE_ACCOUNT\"] = f\"phi4storage{random.randint(0, 999999)}\"\n",
    "\n",
    "# 显示配置\n",
    "print(f\"Resource Group: {os.environ['RESOURCE_GROUP']}\")\n",
    "print(f\"Location: {os.environ['LOCATION']}\")\n",
    "print(f\"AKS Cluster: {os.environ['AKS_CLUSTER_NAME']}\")\n",
    "print(f\"ML Workspace: {os.environ['ML_WORKSPACE_NAME']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b43d238",
   "metadata": {},
   "source": [
    "## VM 环境准备\n",
    "\n",
    "### 1. 创建资源组\n",
    "\n",
    "以下使用 Azure CLI 创建 VM（需要先安装和配置 Azure CLI）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffc66bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "az group create \\\n",
    "    --name $RESOURCE_GROUP \\\n",
    "    --location $LOCATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09625ea",
   "metadata": {},
   "source": [
    "### 2. 创建虚拟机\n",
    "\n",
    "以下使用 Azure CLI 创建 VM（需要先安装和配置 Azure CLI）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9574365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: (ResourceGroupNotFound) Resource group 'test-vm-rg' could not be found.\n",
      "Code: ResourceGroupNotFound\n",
      "Message: Resource group 'test-vm-rg' could not be found.\n"
     ]
    }
   ],
   "source": [
    "# 使用 Azure CLI 创建 VM\n",
    "!az vm create \\\n",
    "  --resource-group $RESOURCE_GROUP \\\n",
    "  --name $VM_NAME \\\n",
    "  --image Ubuntu2204 \\\n",
    "  --size $VM_SIZE \\\n",
    "  --admin-username azureuser \\\n",
    "  --generate-ssh-keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c702549",
   "metadata": {},
   "source": [
    "### 2. 连接到 VM\n",
    "\n",
    "获取 VM 的 IP 地址后，使用 SSH 连接："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf45889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 替换为您的 VM IP 地址\n",
    "VM_IP=\"<VM-IP-ADDRESS>\"\n",
    "!ssh azureuser@$VM_IP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6d8244",
   "metadata": {},
   "source": [
    "### 3. 更新系统\n",
    "\n",
    "运行以下命令更新系统并安装基础工具："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dd8f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新包列表和系统\n",
    "!sudo apt update && sudo apt upgrade -y\n",
    "\n",
    "# 安装基础工具\n",
    "!sudo apt install -y build-essential git wget curl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236acdaa",
   "metadata": {},
   "source": [
    "## 安装依赖\n",
    "\n",
    "### 1. 安装 Python 和 pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d7b974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装 Python 3.10\n",
    "!sudo apt install -y python3.10 python3.10-venv python3.10-dev python3-pip\n",
    "\n",
    "# 创建 Python 虚拟环境\n",
    "!python3.10 -m venv phi4-env\n",
    "!source phi4-env/bin/activate\n",
    "\n",
    "# 升级 pip\n",
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f665a9b0",
   "metadata": {},
   "source": [
    "### 2. 安装 CUDA（如果使用 GPU）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc1470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载并安装 CUDA 12.1\n",
    "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb\n",
    "!sudo dpkg -i cuda-keyring_1.1-1_all.deb\n",
    "!sudo apt-get update\n",
    "!sudo apt-get -y install cuda-12-1\n",
    "\n",
    "# 添加 CUDA 到环境变量\n",
    "%%bash\n",
    "echo 'export PATH=/usr/local/cuda-12.1/bin:$PATH' >> ~/.bashrc\n",
    "echo 'export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc\n",
    "source ~/.bashrc\n",
    "\n",
    "# 验证 CUDA 安装\n",
    "!nvcc --version\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da3e964",
   "metadata": {},
   "source": [
    "### 3. 安装 PyTorch 和相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b3640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装 PyTorch（GPU 版本）\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# 安装 Transformers 和相关库\n",
    "%pip install transformers accelerate bitsandbytes\n",
    "%pip install sentencepiece protobuf\n",
    "%pip install gradio fastapi uvicorn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92a0b43",
   "metadata": {},
   "source": [
    "## 下载模型\n",
    "\n",
    "### 1. 使用 Hugging Face CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0d2c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装 Hugging Face CLI\n",
    "%pip install huggingface-hub\n",
    "\n",
    "# 登录 Hugging Face（需要账号）\n",
    "!huggingface-cli login\n",
    "\n",
    "# 创建模型目录\n",
    "!mkdir -p ~/models/phi-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fca248f",
   "metadata": {},
   "source": [
    "### 2. 使用 Python 脚本下载\n",
    "\n",
    "创建下载脚本 `download_phi4.py`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b367b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile download_phi4.py\n",
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "\n",
    "# 设置模型保存路径\n",
    "model_path = os.path.expanduser(\"~/models/phi-4\")\n",
    "\n",
    "# 下载模型\n",
    "snapshot_download(\n",
    "    repo_id=\"microsoft/Phi-4\",\n",
    "    local_dir=model_path,\n",
    "    cache_dir=model_path,\n",
    "    local_dir_use_symlinks=False\n",
    ")\n",
    "\n",
    "print(f\"模型已下载到: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7e16c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行下载脚本\n",
    "!python download_phi4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483b8096",
   "metadata": {},
   "source": [
    "## 部署模型\n",
    "\n",
    "### 1. 创建模型加载脚本\n",
    "\n",
    "创建 `phi4_server.py`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d6c5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile phi4_server.py\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "import logging\n",
    "\n",
    "# 配置日志\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 初始化 FastAPI\n",
    "app = FastAPI(title=\"Phi-4 Model API\")\n",
    "\n",
    "# 定义请求和响应模型\n",
    "class GenerationRequest(BaseModel):\n",
    "    prompt: str\n",
    "    max_length: int = 512\n",
    "    temperature: float = 0.7\n",
    "    top_p: float = 0.9\n",
    "    top_k: int = 50\n",
    "\n",
    "class GenerationResponse(BaseModel):\n",
    "    generated_text: str\n",
    "    prompt: str\n",
    "\n",
    "# 全局变量存储模型和分词器\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"加载 Phi-4 模型\"\"\"\n",
    "    global model, tokenizer\n",
    "    \n",
    "    model_path = \"/home/azureuser/models/phi-4\"\n",
    "    \n",
    "    logger.info(\"正在加载模型...\")\n",
    "    \n",
    "    # 加载分词器\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # 检测可用设备\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"使用设备: {device}\")\n",
    "    \n",
    "    # 加载模型\n",
    "    if torch.cuda.is_available():\n",
    "        # GPU 加载，使用半精度\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    else:\n",
    "        # CPU 加载\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float32\n",
    "        )\n",
    "        model = model.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    logger.info(\"模型加载完成！\")\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    \"\"\"启动时加载模型\"\"\"\n",
    "    load_model()\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    \"\"\"健康检查端点\"\"\"\n",
    "    return {\"message\": \"Phi-4 Model API is running\"}\n",
    "\n",
    "@app.post(\"/generate\", response_model=GenerationResponse)\n",
    "async def generate_text(request: GenerationRequest):\n",
    "    \"\"\"生成文本端点\"\"\"\n",
    "    try:\n",
    "        # 编码输入\n",
    "        inputs = tokenizer(\n",
    "            request.prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        # 移动到正确的设备\n",
    "        device = next(model.parameters()).device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # 生成文本\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=request.max_length,\n",
    "                temperature=request.temperature,\n",
    "                top_p=request.top_p,\n",
    "                top_k=request.top_k,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # 解码输出\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        return GenerationResponse(\n",
    "            generated_text=generated_text,\n",
    "            prompt=request.prompt\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"生成文本时出错: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528af8c5",
   "metadata": {},
   "source": [
    "### 2. 创建 Gradio Web UI（可选）\n",
    "\n",
    "创建 `phi4_gradio.py`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ae1f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile phi4_gradio.py\n",
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 加载模型\n",
    "def load_model():\n",
    "    model_path = \"/home/azureuser/models/phi-4\"\n",
    "    \n",
    "    logger.info(\"正在加载 Phi-4 模型...\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float32\n",
    "        )\n",
    "        model = model.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    logger.info(\"模型加载完成！\")\n",
    "    \n",
    "    return model, tokenizer, device\n",
    "\n",
    "# 生成函数\n",
    "def generate_response(prompt, max_length, temperature, top_p):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# 加载模型\n",
    "model, tokenizer, device = load_model()\n",
    "\n",
    "# 创建 Gradio 界面\n",
    "iface = gr.Interface(\n",
    "    fn=generate_response,\n",
    "    inputs=[\n",
    "        gr.Textbox(\n",
    "            lines=5,\n",
    "            label=\"输入提示词\",\n",
    "            placeholder=\"请输入您的问题或提示...\"\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            minimum=50,\n",
    "            maximum=1024,\n",
    "            value=256,\n",
    "            step=50,\n",
    "            label=\"最大长度\"\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            minimum=0.1,\n",
    "            maximum=2.0,\n",
    "            value=0.7,\n",
    "            step=0.1,\n",
    "            label=\"温度\"\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            minimum=0.1,\n",
    "            maximum=1.0,\n",
    "            value=0.9,\n",
    "            step=0.05,\n",
    "            label=\"Top-p\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=gr.Textbox(\n",
    "        lines=10,\n",
    "        label=\"生成的文本\"\n",
    "    ),\n",
    "    title=\"Phi-4 模型演示\",\n",
    "    description=\"使用 Phi-4 模型生成文本\",\n",
    "    examples=[\n",
    "        [\"请解释什么是人工智能？\", 256, 0.7, 0.9],\n",
    "        [\"写一首关于春天的诗\", 256, 0.9, 0.95],\n",
    "        [\"如何学习编程？\", 512, 0.7, 0.9]\n",
    "    ]\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch(server_name=\"0.0.0.0\", server_port=7860)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0cfb79",
   "metadata": {},
   "source": [
    "### 3. 创建系统服务（可选）\n",
    "\n",
    "创建 systemd 服务文件 `/etc/systemd/system/phi4.service`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c3ecde",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /etc/systemd/system/phi4.service\n",
    "[Unit]\n",
    "Description=Phi-4 Model Service\n",
    "After=network.target\n",
    "\n",
    "[Service]\n",
    "Type=simple\n",
    "User=azureuser\n",
    "WorkingDirectory=/home/azureuser\n",
    "Environment=\"PATH=/home/azureuser/phi4-env/bin\"\n",
    "ExecStart=/home/azureuser/phi4-env/bin/python /home/azureuser/phi4_server.py\n",
    "Restart=on-failure\n",
    "RestartSec=10\n",
    "\n",
    "[Install]\n",
    "WantedBy=multi-user.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea3d5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 启动服务\n",
    "!sudo systemctl daemon-reload\n",
    "!sudo systemctl enable phi4.service\n",
    "!sudo systemctl start phi4.service\n",
    "!sudo systemctl status phi4.service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9187514",
   "metadata": {},
   "source": [
    "## 测试验证\n",
    "\n",
    "### 1. 测试 API 端点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eff425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试健康检查\n",
    "!curl http://localhost:8000/\n",
    "\n",
    "# 测试文本生成\n",
    "%%bash\n",
    "curl -X POST \"http://localhost:8000/generate\" \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\n",
    "       \"prompt\": \"什么是人工智能？\",\n",
    "       \"max_length\": 256,\n",
    "       \"temperature\": 0.7\n",
    "     }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caebc5b6",
   "metadata": {},
   "source": [
    "### 2. Python 客户端测试\n",
    "\n",
    "创建 `test_client.py`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69c0637",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_client.py\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# API 端点\n",
    "url = \"http://localhost:8000/generate\"\n",
    "\n",
    "# 测试用例\n",
    "test_prompts = [\n",
    "    \"请解释量子计算的基本原理\",\n",
    "    \"写一个 Python 函数计算斐波那契数列\",\n",
    "    \"描述一下未来的智能城市\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\n提示词: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    response = requests.post(\n",
    "        url,\n",
    "        json={\n",
    "            \"prompt\": prompt,\n",
    "            \"max_length\": 256,\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(result[\"generated_text\"])\n",
    "    else:\n",
    "        print(f\"错误: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ddd108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行测试客户端\n",
    "!python test_client.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96908f9",
   "metadata": {},
   "source": [
    "## 性能优化\n",
    "\n",
    "### 1. 量化优化\n",
    "\n",
    "使用 bitsandbytes 进行 8-bit 量化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209c54ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile quantization_example.py\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# 配置 8-bit 量化\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# 加载量化模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebcf351",
   "metadata": {},
   "source": [
    "### 2. 批处理优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3fd7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile batch_processing.py\n",
    "def batch_generate(prompts, batch_size=4):\n",
    "    \"\"\"批量生成文本\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i in range(0, len(prompts), batch_size):\n",
    "        batch = prompts[i:i+batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=256,\n",
    "                temperature=0.7,\n",
    "                do_sample=True\n",
    "            )\n",
    "        \n",
    "        for output in outputs:\n",
    "            text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "            results.append(text)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2d3b73",
   "metadata": {},
   "source": [
    "### 3. 缓存优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eb3018",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile caching.py\n",
    "from functools import lru_cache\n",
    "import hashlib\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def cached_generate(prompt_hash, max_length, temperature):\n",
    "    \"\"\"缓存生成结果\"\"\"\n",
    "    # 实际生成逻辑\n",
    "    pass\n",
    "\n",
    "def generate_with_cache(prompt, **kwargs):\n",
    "    \"\"\"带缓存的生成函数\"\"\"\n",
    "    prompt_hash = hashlib.md5(prompt.encode()).hexdigest()\n",
    "    return cached_generate(prompt_hash, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d978495",
   "metadata": {},
   "source": [
    "## 常见问题\n",
    "\n",
    "### 1. 内存不足"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00e891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 CPU offload\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"offload\",\n",
    "    offload_state_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61ea528",
   "metadata": {},
   "source": [
    "### 2. GPU 驱动问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb96c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新安装 NVIDIA 驱动\n",
    "!sudo apt-get purge nvidia-*\n",
    "!sudo apt-get autoremove\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install nvidia-driver-525\n",
    "!sudo reboot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e6199e",
   "metadata": {},
   "source": [
    "### 3. 模型下载失败"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb9451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用镜像加速\n",
    "!export HF_ENDPOINT=https://hf-mirror.com\n",
    "\n",
    "# 或使用代理\n",
    "!export https_proxy=http://your-proxy:port\n",
    "!export http_proxy=http://your-proxy:port"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
